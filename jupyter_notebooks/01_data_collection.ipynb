{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data from Kaggle and prepare it for further processes.\n",
        "\n",
        "## Inputs\n",
        "* [https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor](https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor)\n",
        "*   Kaggle JSON file - the authentication token. \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate Dataset: \n",
        "    * input/\n",
        "    * └── Brain Tumor/ (Image files)\n",
        "    * ├── Brain Tumor.csv\n",
        "    * └── bt_dataset_t3.csv\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        "Brain Tumor Data Set\n",
        "- This dataset includes the Brain MRI image files and two csv files.\n",
        "\n",
        "- The csv files contain brain tumor feature dataset including five first-order features and eight texture features with the target level (in the column Class).\n",
        "\n",
        "    - First Order Features\n",
        "        - Mean\n",
        "        - Variance\n",
        "        - Standard Deviation\n",
        "        - Skewness\n",
        "        - Kurtosis\n",
        "\n",
        "    - Second Order Features\n",
        "        - Contrast\n",
        "        - Energy\n",
        "        - ASM (Angular second moment)\n",
        "        - Entropy\n",
        "        - Homogeneity\n",
        "        - Dissimilarity\n",
        "        - Correlation\n",
        "        - Coarseness \n",
        "\n",
        "- Image column defines image name and Class column defines either the image has tumor or not (1 = Tumor, 0 = Non-Tumor). These two feature are the ones we will take into consideration while classifying the images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit==0.85.0 (from -r ../requirements.txt (line 2))\n",
            "  Using cached streamlit-0.85.0-py2.py3-none-any.whl (7.9 MB)\n",
            "Collecting altair<5 (from -r ../requirements.txt (line 3))\n",
            "  Using cached altair-4.2.2-py3-none-any.whl (813 kB)\n",
            "Collecting astor (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting attrs (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting base58 (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Using cached base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting blinker (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for blinker from https://files.pythonhosted.org/packages/fa/2a/7f3714cbc6356a0efec525ce7a0613d581072ed6eb53eb7b9754f33db807/blinker-1.7.0-py3-none-any.whl.metadata\n",
            "  Using cached blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting cachetools>=4.0 (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for cachetools>=4.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
            "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting click<8.0,>=7.0 (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "Collecting numpy (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/98/5d/5738903efe0ecb73e51eb44feafba32bdba2081263d40c5043568ff60faf/numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: packaging in /home/tom/codeinstitute/brain-tumor-detect/venv/lib/python3.8/site-packages (from streamlit==0.85.0->-r ../requirements.txt (line 2)) (23.2)\n",
            "Collecting pandas>=0.21.0 (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for pandas>=0.21.0 from https://files.pythonhosted.org/packages/f8/7f/5b047effafbdd34e52c9e2d7e44f729a0655efafb22198c45cf692cdc157/pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting pillow>=6.2.0 (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for pillow>=6.2.0 from https://files.pythonhosted.org/packages/1e/74/638f982ab43fb3b19c8a151b1a0065cafefe436f8590c1c57d5fdf2475f1/Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata\n",
            "  Using cached Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting protobuf!=3.11,>=3.6.0 (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for protobuf!=3.11,>=3.6.0 from https://files.pythonhosted.org/packages/ae/5b/7ed02a9b8e752c8f7bca8661779c0275b9e3e6a903a3045e6da51f796dda/protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pyarrow (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/c0/c9/c8d2157aa20e257dbf3c1fc82879f9a8a3d5128aabc4fa41a15d4523a93c/pyarrow-14.0.2-cp38-cp38-manylinux_2_28_x86_64.whl.metadata\n",
            "  Using cached pyarrow-14.0.2-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pydeck>=0.1.dev5 (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "Requirement already satisfied: python-dateutil in /home/tom/codeinstitute/brain-tumor-detect/venv/lib/python3.8/site-packages (from streamlit==0.85.0->-r ../requirements.txt (line 2)) (2.8.2)\n",
            "Collecting requests (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting toml (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tornado>=5.0 in /home/tom/codeinstitute/brain-tumor-detect/venv/lib/python3.8/site-packages (from streamlit==0.85.0->-r ../requirements.txt (line 2)) (6.4)\n",
            "Collecting tzlocal (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for tzlocal from https://files.pythonhosted.org/packages/97/3f/c4c51c55ff8487f2e6d0e618dba917e3c3ee2caae6cf0fbb59c9b1876f2e/tzlocal-5.2-py3-none-any.whl.metadata\n",
            "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting validators (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for validators from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n",
            "  Using cached validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting gitpython (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for gitpython from https://files.pythonhosted.org/packages/8d/c4/82b858fb6483dfb5e338123c154d19c043305b01726a67d89532b8f8f01b/GitPython-3.1.40-py3-none-any.whl.metadata\n",
            "  Using cached GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting watchdog (from streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Using cached watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "Collecting entrypoints (from altair<5->-r ../requirements.txt (line 3))\n",
            "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Collecting jinja2 (from altair<5->-r ../requirements.txt (line 3))\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting jsonschema>=3.0 (from altair<5->-r ../requirements.txt (line 3))\n",
            "  Obtaining dependency information for jsonschema>=3.0 from https://files.pythonhosted.org/packages/0f/ed/0058234d8dd2b1fc6beeea8eab945191a05e9d391a63202f49fe23327586/jsonschema-4.20.0-py3-none-any.whl.metadata\n",
            "  Using cached jsonschema-4.20.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting toolz (from altair<5->-r ../requirements.txt (line 3))\n",
            "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
            "Collecting importlib-resources>=1.4.0 (from jsonschema>=3.0->altair<5->-r ../requirements.txt (line 3))\n",
            "  Obtaining dependency information for importlib-resources>=1.4.0 from https://files.pythonhosted.org/packages/93/e8/facde510585869b5ec694e8e0363ffe4eba067cb357a8398a55f6a1f8023/importlib_resources-6.1.1-py3-none-any.whl.metadata\n",
            "  Using cached importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<5->-r ../requirements.txt (line 3))\n",
            "  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/ee/07/44bd408781594c4d0a027666ef27fab1e441b109dc3b76b4f836f8fd04fe/jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema>=3.0->altair<5->-r ../requirements.txt (line 3))\n",
            "  Using cached pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<5->-r ../requirements.txt (line 3))\n",
            "  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/b4/11/d121780c173336c9bc3a5b8240ed31f518957cc22f6311c76259cb0fcf32/referencing-0.32.0-py3-none-any.whl.metadata\n",
            "  Using cached referencing-0.32.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<5->-r ../requirements.txt (line 3))\n",
            "  Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/ea/cc/436512512dc03fc6fdeff38ed50037219b0966c9b904092fef18a6aae3b5/rpds_py-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached rpds_py-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting pytz>=2020.1 (from pandas>=0.21.0->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
            "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas>=0.21.0->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->altair<5->-r ../requirements.txt (line 3))\n",
            "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/de/e2/32c14301bb023986dff527a49325b6259cab4ebb4633f69de54af312fc45/MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/tom/codeinstitute/brain-tumor-detect/venv/lib/python3.8/site-packages (from python-dateutil->streamlit==0.85.0->-r ../requirements.txt (line 2)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
            "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/3d/09/d82fe4a34c5f0585f9ea1df090e2a71eb9bb1e469723053e1ee9f57c16f3/charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl.metadata\n",
            "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/96/94/c31f58c7a7f470d5665935262ebd7455c7e4c7782eb525658d3dbf4b9403/urllib3-2.1.0-py3-none-any.whl.metadata\n",
            "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/64/62/428ef076be88fa93716b576e4a01f919d25968913e817077a386fcbe4f42/certifi-2023.11.17-py3-none-any.whl.metadata\n",
            "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting backports.zoneinfo (from tzlocal->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Using cached backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->streamlit==0.85.0->-r ../requirements.txt (line 2))\n",
            "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
            "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/tom/codeinstitute/brain-tumor-detect/venv/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair<5->-r ../requirements.txt (line 3)) (3.17.0)\n",
            "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
            "Using cached jsonschema-4.20.0-py3-none-any.whl (84 kB)\n",
            "Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "Using cached pyarrow-14.0.2-cp38-cp38-manylinux_2_28_x86_64.whl (38.1 MB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
            "Using cached validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
            "Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
            "Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Using cached MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "Using cached referencing-0.32.0-py3-none-any.whl (26 kB)\n",
            "Using cached rpds_py-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pytz, watchdog, validators, urllib3, tzdata, toolz, toml, smmap, rpds-py, protobuf, pkgutil-resolve-name, pillow, numpy, MarkupSafe, importlib-resources, idna, entrypoints, click, charset-normalizer, certifi, cachetools, blinker, base58, backports.zoneinfo, attrs, astor, tzlocal, requests, referencing, pyarrow, pandas, jinja2, gitdb, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
            "Successfully installed MarkupSafe-2.1.3 altair-4.2.2 astor-0.8.1 attrs-23.1.0 backports.zoneinfo-0.2.1 base58-2.1.1 blinker-1.7.0 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 click-7.1.2 entrypoints-0.4 gitdb-4.0.11 gitpython-3.1.40 idna-3.6 importlib-resources-6.1.1 jinja2-3.1.2 jsonschema-4.20.0 jsonschema-specifications-2023.12.1 numpy-1.24.4 pandas-2.0.3 pillow-10.1.0 pkgutil-resolve-name-1.3.10 protobuf-4.25.1 pyarrow-14.0.2 pydeck-0.8.1b0 pytz-2023.3.post1 referencing-0.32.0 requests-2.31.0 rpds-py-0.15.2 smmap-5.0.1 streamlit-0.85.0 toml-0.10.2 toolz-0.12.0 tzdata-2023.3 tzlocal-5.2 urllib3-2.1.0 validators-0.22.0 watchdog-3.0.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Because of the Jupyter notebooks being in a subfolder, we need to change the directory for the code's execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/tom/codeinstitute/brain-tumor-detect'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/tom/codeinstitute/brain-tumor-detect'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Setup Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle==1.5.12\n",
            "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10 in ./venv/lib/python3.8/site-packages (from kaggle==1.5.12) (1.16.0)\n",
            "Requirement already satisfied: certifi in ./venv/lib/python3.8/site-packages (from kaggle==1.5.12) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in ./venv/lib/python3.8/site-packages (from kaggle==1.5.12) (2.8.2)\n",
            "Requirement already satisfied: requests in ./venv/lib/python3.8/site-packages (from kaggle==1.5.12) (2.31.0)\n",
            "Collecting tqdm (from kaggle==1.5.12)\n",
            "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting python-slugify (from kaggle==1.5.12)\n",
            "  Using cached python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: urllib3 in ./venv/lib/python3.8/site-packages (from kaggle==1.5.12) (2.1.0)\n",
            "Collecting text-unidecode>=1.3 (from python-slugify->kaggle==1.5.12)\n",
            "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests->kaggle==1.5.12) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests->kaggle==1.5.12) (3.6)\n",
            "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: text-unidecode, tqdm, python-slugify, kaggle\n",
            "Successfully installed kaggle-1.5.12 python-slugify-8.0.1 text-unidecode-1.3 tqdm-4.66.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "setup Kaggle details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle json file and directory setup\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kaggle download settings and download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading brain-tumor.zip to input\n",
            "100%|██████████████████████████████████████| 14.0M/14.0M [00:01<00:00, 18.2MB/s]\n",
            "100%|██████████████████████████████████████| 14.0M/14.0M [00:01<00:00, 13.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "KAGGLE_DATASET_URL = 'jakeshbohaju/brain-tumor'\n",
        "DESTINATION_FOLDER = 'input/'\n",
        "! kaggle datasets download -d $KAGGLE_DATASET_URL -p $DESTINATION_FOLDER\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unzip the downloaded file, and delete the zip file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unzip the downloaded file, and delete the zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(DESTINATION_FOLDER + '/brain-tumor.zip' , 'r') as zip_ref:\n",
        "    zip_ref.extractall(DESTINATION_FOLDER)\n",
        "\n",
        "os.remove(DESTINATION_FOLDER + '/brain-tumor.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rename directories and files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Brain Tumor'  'Brain Tumor.csv'   bt_dataset_t3.csv\n"
          ]
        }
      ],
      "source": [
        "! ls input/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "! mv 'input/Brain Tumor.csv' input/brain-tumor.csv\n",
        "! mv input/Brain\\ Tumor/ input/brain-tumor/\n",
        "! mv input/brain-tumor/Brain\\ Tumor/ input/brain-tumor/brain-tumor/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "1. Sort the image files into tumor and non-tumor directories\n",
        "2. Remove non image files\n",
        "3. Remove empty directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "brain-tumor\n"
          ]
        }
      ],
      "source": [
        "! ls input/brain-tumor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change the dir structure of the input folder\n",
        "! mkdir input/mri-brain-tumor/\n",
        "! cp input/brain-tumor/brain-tumor/* input/mri-brain-tumor/\n",
        "! rm -rf input/brain-tumor/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3762 entries, 0 to 3761\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Image   3762 non-null   object\n",
            " 1   Class   3762 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 58.9+ KB\n"
          ]
        }
      ],
      "source": [
        "# classify images according to the target 'Class'\n",
        "import pandas as pd\n",
        "df = pd.read_csv('input/brain-tumor.csv')\n",
        "\n",
        "# take out Image and Class only into a new data set\n",
        "new_df = df[['Image', 'Class']]\n",
        "new_df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# make new directories mri-tumor and mri-non-tumor in input\n",
        "os.mkdir('input/mri-tumor/')\n",
        "os.mkdir('input/mri-non-tumor/')\n",
        "\n",
        "# move files according to the class\n",
        "for index, row in new_df.iterrows():\n",
        "    image_file = row['Image'] + '.jpg'\n",
        "    image_class = row['Class']\n",
        "    # save the image into the folder according to the class\n",
        "    if image_class == 0:\n",
        "        # save the image into the folder according to the class\n",
        "        shutil.move('input/mri-brain-tumor/'+ image_file, 'input/mri-non-tumor/')\n",
        "    else:\n",
        "        # save the image into the folder according to the class\n",
        "        shutil.move('input/mri-brain-tumor/'+ image_file, 'input/mri-tumor/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove non image files and empty folders\n",
        "! rm input/*.csv\n",
        "! rm -rf input/mri-brain-tumor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mri-tumor', 'mri-non-tumor']"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir('input')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
